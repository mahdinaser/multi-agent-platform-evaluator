# Extended Research Experiment Engine Configuration
# PVLDB Paper - Multi-Agent Multi-Platform Evaluation Framework
# Extended with: 7 platforms, 11 agents, benchmarks, scalability tests, confidence intervals

experiment:
  seed: 42
  num_runs: 10  # Run each configuration 10 times for statistical significance
  warmup_runs: 3  # Warm-up runs to exclude (not counted in metrics)
  parallel: false
  quick_mode: false  # Run all platforms for comprehensive evaluation
  max_data_size: 0  # 0 = no limit
  confidence_level: 0.95  # For confidence interval calculation
  
data:
  tabular:
    sizes: [50000, 500000, 1000000]
    num_categorical: 5
    num_numeric: 10
  
  logs:
    num_events: 100000
    num_users: 1000
    num_event_types: 20
  
  vectors:
    num_vectors: 10000
    dimension: 128
  
  timeseries:
    num_samples: 100000
    frequency: "1min"
  
  text:
    num_documents: 5000
    avg_length: 50

# NEW: Scalability tests
scalability:
  enabled: true
  data_sizes: [10000, 50000, 100000, 500000, 1000000, 5000000, 10000000]
  experiment_types: [scan, filter, aggregate]  # Test subset of queries
  agents: [cost_model, llm, oracle]  # Test subset of agents
  platforms: [pandas, duckdb, polars]  # Test subset of platforms

# NEW: Benchmarks
benchmarks:
  tpch:
    enabled: true
    scale_factors: [1, 10]  # SF1 (1GB), SF10 (10GB)
    queries: [1, 3, 6, 12]  # Representative TPC-H queries
  
  nyc_taxi:
    enabled: false  # Set to true to enable NYC Taxi benchmark
    months: [1]  # Number of months of data (1 month ~ 15M rows)
    queries: [time_window, aggregate, filter]

platforms:
  - pandas
  - duckdb
  - polars  # NEW: Fast DataFrame library
  - sqlite
  - faiss
  - annoy
  - baseline

agents:
  - rule_based
  - bandit  # UCB1
  - cost_model
  - llm
  - hybrid
  # NEW: Baseline agents
  - random  # Random selection (performance floor)
  - oracle  # Optimal selection (performance ceiling)
  - static_best  # Historical best platform
  - round_robin  # Fair cycling
  # NEW: Advanced agents
  - linucb  # Contextual bandit with features
  - thompson  # Thompson sampling (Bayesian)

llm_config:
  use_local: true
  use_ollama: true
  model_name: "llama2"
  api_key: null
  # NEW: Multi-model comparison
  compare_models: false  # Set true to test multiple models
  models_to_compare: ["llama2", "llama3", "phi"]  # Compare multiple LLMs

experiments:
  - scan
  - filter
  - aggregate
  - join
  - time_window
  - vector_knn
  - text_similarity

metrics:
  track_latency: true
  track_memory: true
  track_cpu: true
  track_correctness: true
  track_stability: true
  track_decision_time: true  # NEW: Track agent decision time separately
  compute_confidence_intervals: true  # NEW: Calculate 95% CI for all metrics
  track_exploration_rate: true  # NEW: Track exploration vs exploitation for bandits

